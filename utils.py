"""
å·¥å…·å‡½æ•°ï¼šPDFå¤„ç† + æ¨¡å‹æ¨ç† - Phase 3.2 æµå¼ç‰ˆæœ¬
"""
import base64
from pathlib import Path
from PIL import Image
import fitz  # PyMuPDF
from openai import OpenAI
import config
from io import BytesIO
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
from typing import Generator, Dict, Any, List, Tuple
from cache_manager import get_cache_manager


def pdf_to_images(pdf_path):
    """PDF è½¬å›¾ç‰‡"""
    doc = fitz.open(pdf_path)
    images = []
    
    for page_num in range(len(doc)):
        page = doc[page_num]
        mat = fitz.Matrix(config.PDF_DPI / 72, config.PDF_DPI / 72)
        pix = page.get_pixmap(matrix=mat)
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        images.append(img)
    
    doc.close()
    return images


def resize_image_if_needed(image):
    """è°ƒæ•´å›¾åƒå¤§å°"""
    width, height = image.size
    pixels = width * height
    
    if pixels > config.IMAGE_MAX_PIXELS:
        scale = (config.IMAGE_MAX_PIXELS / pixels) ** 0.5
        new_width = int(width * scale)
        new_height = int(height * scale)
        image = image.resize((new_width, new_height), Image.LANCZOS)
    
    return image


def image_to_base64(image):
    """å›¾ç‰‡è½¬ Base64"""
    if isinstance(image, (str, Path)):
        image = Image.open(image)
    
    image = resize_image_if_needed(image)
    
    buffer = BytesIO()
    if image.mode in ('RGBA', 'LA', 'P'):
        image = image.convert('RGB')
    image.save(buffer, format='JPEG', quality=85)
    
    return base64.b64encode(buffer.getvalue()).decode()


def infer_single_image(image, model_key, prompt, temperature, top_p, max_tokens):
    """å•å¼ å›¾ç‰‡æ¨ç†"""
    if model_key not in config.MODELS:
        raise ValueError(f"æœªçŸ¥çš„æ¨¡å‹: {model_key}")
    
    model_config = config.MODELS[model_key]
    img_base64 = image_to_base64(image)
    
    client = OpenAI(
        api_key="dummy",
        base_url=model_config["api_base"],
        timeout=120.0
    )
    
    response = client.chat.completions.create(
        model=model_config["model_id"],
        messages=[{
            "role": "user",
            "content": [
                {
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/jpeg;base64,{img_base64}"
                    }
                },
                {
                    "type": "text",
                    "text": prompt
                }
            ]
        }],
        max_tokens=max_tokens,
        temperature=temperature,
        top_p=top_p,
        extra_body={
            'repetition_penalty': 1.0,
            'top_k': 50,
            'skip_special_tokens': True,
        }
    )
    
    return response.choices[0].message.content


def process_single_page_with_index(idx, image, model_key, prompt, temperature, top_p, max_tokens):
    """å¤„ç†å•é¡µï¼ˆå¸¦ç´¢å¼•ï¼‰"""
    start_time = time.time()
    try:
        result = infer_single_image(image, model_key, prompt, temperature, top_p, max_tokens)
        elapsed = time.time() - start_time
        return (idx, result, elapsed, None)
    except Exception as e:
        elapsed = time.time() - start_time
        return (idx, None, elapsed, str(e))


# ============================================
# Phase 3.2: æµå¼å¤„ç†æ ¸å¿ƒå‡½æ•°
# ============================================

def process_images_streaming(
    images: List[Image.Image],
    model_key: str,
    prompt: str,
    temperature: float,
    top_p: float,
    max_tokens: int
) -> Generator[Dict[str, Any], None, None]:
    """
    æµå¼å¹¶è¡Œå¤„ç†å›¾ç‰‡ï¼ˆPhase 3.2 æ ¸å¿ƒï¼‰
    
    æ¯å®Œæˆä¸€é¡µå°±ç«‹å³è¿”å›ç»“æœ
    
    Args:
        images: å›¾ç‰‡åˆ—è¡¨
        å…¶ä»–å‚æ•°: æ¨¡å‹æ¨ç†å‚æ•°
    
    Yields:
        {
            "page_num": 1,           # å½“å‰å®Œæˆçš„é¡µç 
            "total_pages": 10,       # æ€»é¡µæ•°
            "result": "markdown",    # å½“å‰é¡µç»“æœ
            "completed": 3,          # å·²å®Œæˆé¡µæ•°
            "progress": 0.3,         # è¿›åº¦ (0-1)
            "elapsed": 15.2,         # å·²ç”¨æ—¶é—´
            "eta": 35.1,            # é¢„è®¡å‰©ä½™æ—¶é—´
            "status": "âœ… Page 3/10" # çŠ¶æ€æ–‡æœ¬
        }
    """
    total = len(images)
    completed_count = 0
    elapsed_times = []
    start_time = time.time()
    
    # å­˜å‚¨ç»“æœï¼ˆä¿æŒé¡ºåºï¼‰
    results = {}
    
    # å•é¡µç›´æ¥å¤„ç†
    if total < config.PARALLEL_MIN_PAGES:
        for idx, img in enumerate(images):
            page_start = time.time()
            _, result, page_elapsed, error = process_single_page_with_index(
                idx, img, model_key, prompt, temperature, top_p, max_tokens
            )
            
            completed_count += 1
            elapsed_times.append(page_elapsed)
            results[idx] = result if error is None else f"Error: {error}"
            
            # è®¡ç®— ETA
            avg_time = sum(elapsed_times) / len(elapsed_times)
            remaining = total - completed_count
            eta = avg_time * remaining
            
            # âœ… ç«‹å³è¿”å›å½“å‰é¡µç»“æœ
            yield {
                "page_num": idx + 1,
                "total_pages": total,
                "result": results[idx],
                "completed": completed_count,
                "progress": completed_count / total,
                "elapsed": time.time() - start_time,
                "eta": eta,
                "status": f"âœ… Page {idx + 1}/{total} completed ({page_elapsed:.1f}s)"
            }
        return
    
    # å¤šé¡µå¹¶è¡Œå¤„ç†
    with ThreadPoolExecutor(max_workers=config.MAX_WORKERS) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_idx = {
            executor.submit(
                process_single_page_with_index,
                idx, img, model_key, prompt, temperature, top_p, max_tokens
            ): idx
            for idx, img in enumerate(images)
        }
        
        # å®æ—¶æ”¶é›†ç»“æœ
        for future in as_completed(future_to_idx):
            idx, result, page_elapsed, error = future.result()
            completed_count += 1
            elapsed_times.append(page_elapsed)
            
            # ä¿å­˜ç»“æœ
            results[idx] = result if error is None else f"Error: {error}"
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            avg_time = sum(elapsed_times) / len(elapsed_times)
            remaining = total - completed_count
            eta = avg_time * remaining
            total_elapsed = time.time() - start_time
            
            # âœ… ç«‹å³è¿”å›å½“å‰é¡µç»“æœ
            yield {
                "page_num": idx + 1,
                "total_pages": total,
                "result": results[idx],
                "completed": completed_count,
                "progress": completed_count / total,
                "elapsed": total_elapsed,
                "eta": eta,
                "status": f"âœ… Page {idx + 1}/{total} completed ({page_elapsed:.1f}s, ETA: {eta:.1f}s)"
            }


def merge_results_ordered(results: Dict[int, str], total: int) -> str:
    """
    æŒ‰é¡ºåºåˆå¹¶ç»“æœ
    
    Args:
        results: {é¡µç ç´¢å¼•: markdownç»“æœ}
        total: æ€»é¡µæ•°
    
    Returns:
        åˆå¹¶åçš„ markdown
    """
    ordered_results = []
    for i in range(total):
        if i in results:
            ordered_results.append(f"## Page {i + 1}\n\n{results[i]}")
        else:
            ordered_results.append(f"## Page {i + 1}\n\nâ³ Processing...")
    
    return "\n\n---\n\n".join(ordered_results)


def process_document_streaming(
    file_path: str,
    model_key: str,
    prompt: str,
    temperature: float,
    top_p: float,
    max_tokens: int
) -> Generator[Tuple[List[Image.Image], str, str], None, None]:
    """
    æµå¼å¤„ç†æ–‡æ¡£ï¼ˆPhase 3.2 ä¸»æ¥å£ï¼‰
    
    æ¯å®Œæˆä¸€é¡µå°±è¿”å›å½“å‰çŠ¶æ€
    
    Args:
        file_path: æ–‡ä»¶è·¯å¾„
        å…¶ä»–å‚æ•°: æ¨ç†å‚æ•°
    
    Yields:
        (images, status, markdown)
        - images: å·²å¤„ç†çš„å›¾ç‰‡åˆ—è¡¨
        - status: çŠ¶æ€æ–‡æœ¬
        - markdown: å½“å‰ç´¯ç§¯çš„ç»“æœ
    """
    file_path = Path(file_path)
    
    # åŠ è½½å›¾ç‰‡
    if file_path.suffix.lower() == '.pdf':
        images = pdf_to_images(file_path)
    else:
        images = [Image.open(file_path)]
    
    total = len(images)
    
    # åˆå§‹çŠ¶æ€
    yield (
        images,
        f"ğŸ“„ Loaded {total} page(s), starting processing...",
        ""
    )
    
    # æ”¶é›†æ‰€æœ‰ç»“æœ
    all_results = {}
    
    # æµå¼å¤„ç†
    for update in process_images_streaming(
        images, model_key, prompt, temperature, top_p, max_tokens
    ):
        page_idx = update["page_num"] - 1
        all_results[page_idx] = update["result"]
        
        # æ„å»ºçŠ¶æ€æ–‡æœ¬
        progress_bar = "â–ˆ" * int(update["progress"] * 20) + "â–‘" * (20 - int(update["progress"] * 20))
        status = f"""ğŸ”„ Processing: {update['completed']}/{update['total_pages']} pages

{progress_bar} {update['progress']*100:.1f}%

â±ï¸  Elapsed: {update['elapsed']:.1f}s
â° ETA: {update['eta']:.1f}s

{update['status']}"""
        
        # åˆå¹¶å½“å‰ç»“æœ
        if total == 1:
            markdown = all_results.get(0, "")
        else:
            markdown = merge_results_ordered(all_results, total)
        
        # âœ… è¿”å›å½“å‰çŠ¶æ€
        yield (images, status, markdown)
    
    # æœ€ç»ˆçŠ¶æ€
    final_markdown = merge_results_ordered(all_results, total) if total > 1 else all_results.get(0, "")
    
    yield (
        images,
        f"âœ… Completed! {total} page(s) processed successfully.",
        final_markdown
    )


# ============================================
# ä¿ç•™åŸæœ‰æ¥å£ï¼ˆå‘ä¸‹å…¼å®¹ï¼‰
# ============================================

def process_images_parallel(images, model_key, prompt, temperature, top_p, max_tokens, progress=None):
    """å¹¶è¡Œå¤„ç†ï¼ˆéæµå¼ï¼Œä¿ç•™å…¼å®¹ï¼‰"""
    total = len(images)
    
    if total < config.PARALLEL_MIN_PAGES:
        results = []
        for idx, img in enumerate(images):
            if progress is not None:
                progress((idx + 1) / total, desc=f"Processing page {idx + 1}/{total}")
            
            _, result, elapsed, error = process_single_page_with_index(
                idx, img, model_key, prompt, temperature, top_p, max_tokens
            )
            results.append(result if error is None else f"Error: {error}")
        return results
    
    results = [None] * total
    completed_count = 0
    total_time = 0
    
    with ThreadPoolExecutor(max_workers=config.MAX_WORKERS) as executor:
        future_to_idx = {
            executor.submit(
                process_single_page_with_index,
                idx, img, model_key, prompt, temperature, top_p, max_tokens
            ): idx
            for idx, img in enumerate(images)
        }
        
        for future in as_completed(future_to_idx):
            idx, result, elapsed, error = future.result()
            completed_count += 1
            total_time += elapsed
            
            if error is None:
                results[idx] = result
            else:
                results[idx] = f"Error on page {idx + 1}: {error}"
            
            if progress is not None:
                avg_time = total_time / completed_count
                remaining = total - completed_count
                eta = avg_time * remaining
                progress(
                    completed_count / total,
                    desc=f"Completed {completed_count}/{total} pages (ETA: {eta:.1f}s)"
                )
    
    return results


def process_document(file_path, model_key, prompt, temperature, top_p, max_tokens, progress=None):
    """å¤„ç†æ–‡æ¡£ï¼ˆéæµå¼ï¼Œä¿ç•™å…¼å®¹ï¼‰"""
    file_path = Path(file_path)
    
    if file_path.suffix.lower() == '.pdf':
        if progress is not None:
            progress(0, desc="Converting PDF to images...")
        images = pdf_to_images(file_path)
    else:
        images = [Image.open(file_path)]
    
    if config.PARALLEL_ENABLED:
        results = process_images_parallel(
            images, model_key, prompt, temperature, top_p, max_tokens, progress
        )
    else:
        results = []
        for idx, img in enumerate(images):
            if progress is not None:
                progress((idx + 1) / len(images), desc=f"Processing page {idx + 1}/{len(images)}")
            result = infer_single_image(img, model_key, prompt, temperature, top_p, max_tokens)
            results.append(result)
    
    if len(results) > 1:
        markdown = "\n\n---\n\n".join([
            f"## Page {i + 1}\n\n{result}" 
            for i, result in enumerate(results)
        ])
    else:
        markdown = results[0]
    
    return images, markdown


def validate_file(file_path):
    """éªŒè¯æ–‡ä»¶"""
    if not file_path:
        return False, "è¯·ä¸Šä¼ æ–‡ä»¶"
    
    file_path = Path(file_path)
    
    if not file_path.exists():
        return False, "æ–‡ä»¶ä¸å­˜åœ¨"
    
    if file_path.suffix.lower() not in config.ALLOWED_FILE_TYPES:
        return False, f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹ï¼Œä»…æ”¯æŒï¼š{', '.join(config.ALLOWED_FILE_TYPES)}"
    
    size_mb = file_path.stat().st_size / (1024 * 1024)
    if size_mb > config.MAX_FILE_SIZE_MB:
        return False, f"æ–‡ä»¶è¿‡å¤§ï¼ˆ{size_mb:.1f}MBï¼‰ï¼Œæœ€å¤§æ”¯æŒ {config.MAX_FILE_SIZE_MB}MB"
    
    return True, ""


def get_model_choices():
    """è·å–æ¨¡å‹é€‰æ‹©åˆ—è¡¨"""
    return [model["name"] for model in config.MODELS.values()]


def get_model_key_from_name(model_name):
    """ä»æ˜¾ç¤ºåç§°è·å–æ¨¡å‹é”®"""
    for key, model in config.MODELS.items():
        if model["name"] == model_name:
            return key
    return config.DEFAULT_MODEL


def test_model_connection(model_key):
    """æµ‹è¯•æ¨¡å‹ API è¿æ¥"""
    try:
        model_config = config.MODELS[model_key]
        client = OpenAI(
            api_key="dummy",
            base_url=model_config["api_base"],
            timeout=5.0
        )
        
        client.models.list()
        return True, f"âœ… {model_config['name']} è¿æ¥æ­£å¸¸"
    
    except Exception as e:
        return False, f"âŒ {model_config['name']} è¿æ¥å¤±è´¥: {str(e)}"

# ============================================
# Phase 3.3: ç¼“å­˜é…ç½®
# ============================================

def process_document_with_cache(
    file_path: str,
    model_key: str,
    prompt: str,
    temperature: float,
    top_p: float,
    max_tokens: int
) -> Tuple[List[Image.Image], str, bool]:
    """
    å¤„ç†æ–‡æ¡£ï¼ˆå¸¦ç¼“å­˜ï¼‰
    
    Returns:
        (images, markdown, from_cache)
        - images: å›¾ç‰‡åˆ—è¡¨
        - markdown: ç»“æœ
        - from_cache: æ˜¯å¦æ¥è‡ªç¼“å­˜
    """
    if not config.CACHE_ENABLED:
        # ç¼“å­˜æœªå¯ç”¨ï¼Œç›´æ¥å¤„ç†
        images, markdown = process_document(
            file_path, model_key, prompt, temperature, top_p, max_tokens
        )
        return images, markdown, False
    
    # è·å–ç¼“å­˜ç®¡ç†å™¨
    cache_mgr = get_cache_manager()
    
    # ç”Ÿæˆç¼“å­˜é”®
    cache_key = cache_mgr.generate_cache_key(
        file_path, model_key, prompt, temperature, top_p, max_tokens
    )
    
    # å°è¯•ä»ç¼“å­˜è·å–
    cached_result = cache_mgr.get(cache_key)
    
    if cached_result is not None:
        # ç¼“å­˜å‘½ä¸­
        # é‡æ–°åŠ è½½å›¾ç‰‡
        file_path = Path(file_path)
        if file_path.suffix.lower() == '.pdf':
            images = pdf_to_images(file_path)
        else:
            images = [Image.open(file_path)]
        
        markdown = cached_result["markdown"]
        return images, markdown, True
    
    # ç¼“å­˜æœªå‘½ä¸­ï¼Œæ‰§è¡Œæ¨ç†
    images, markdown = process_document(
        file_path, model_key, prompt, temperature, top_p, max_tokens
    )
    
    # ä¿å­˜åˆ°ç¼“å­˜
    result = {
        "markdown": markdown,
        "metadata": {
            "pages": len(images),
            "model": model_key,
            "timestamp": time.time()
        }
    }
    
    cache_mgr.set(
        cache_key,
        result,
        Path(file_path).name,
        model_key,
        temperature,
        top_p,
        max_tokens
    )
    
    return images, markdown, False


def process_document_streaming_with_cache(
    file_path: str,
    model_key: str,
    prompt: str,
    temperature: float,
    top_p: float,
    max_tokens: int
) -> Generator[Tuple[List[Image.Image], str, str, bool], None, None]:
    """
    æµå¼å¤„ç†æ–‡æ¡£ï¼ˆå¸¦ç¼“å­˜ï¼‰
    
    Yields:
        (images, status, markdown, from_cache)
    """
    if not config.CACHE_ENABLED:
        # ç¼“å­˜æœªå¯ç”¨ï¼Œç›´æ¥æµå¼å¤„ç†
        for images, status, markdown in process_document_streaming(
            file_path, model_key, prompt, temperature, top_p, max_tokens
        ):
            yield images, status, markdown, False
        return
    
    # è·å–ç¼“å­˜ç®¡ç†å™¨
    cache_mgr = get_cache_manager()
    
    # ç”Ÿæˆç¼“å­˜é”®
    cache_key = cache_mgr.generate_cache_key(
        file_path, model_key, prompt, temperature, top_p, max_tokens
    )
    
    # å°è¯•ä»ç¼“å­˜è·å–
    cached_result = cache_mgr.get(cache_key)
    
    if cached_result is not None:
        # âœ… ç¼“å­˜å‘½ä¸­ - ç«‹å³è¿”å›
        file_path_obj = Path(file_path)
        if file_path_obj.suffix.lower() == '.pdf':
            images = pdf_to_images(file_path_obj)
        else:
            images = [Image.open(file_path_obj)]
        
        markdown = cached_result["markdown"]
        pages = cached_result["metadata"]["pages"]
        
        status = f"""âš¡ Loaded from cache!

ğŸ“„ Pages: {pages}
ğŸ”¥ Response time: <0.1s
ğŸ’¾ Cache hit!"""
        
        yield images, status, markdown, True
        return
    
    # âŒ ç¼“å­˜æœªå‘½ä¸­ - æ‰§è¡Œæµå¼å¤„ç†
    all_results = {}
    final_images = None
    final_markdown = ""
    
    for images, status, markdown in process_document_streaming(
        file_path, model_key, prompt, temperature, top_p, max_tokens
    ):
        final_images = images
        final_markdown = markdown
        yield images, status, markdown, False
    
    # å¤„ç†å®Œæˆï¼Œä¿å­˜åˆ°ç¼“å­˜
    if final_images is not None:
        result = {
            "markdown": final_markdown,
            "metadata": {
                "pages": len(final_images),
                "model": model_key,
                "timestamp": time.time()
            }
        }
        
        cache_mgr.set(
            cache_key,
            result,
            Path(file_path).name,
            model_key,
            temperature,
            top_p,
            max_tokens
        )


def get_cache_stats():
    """è·å–ç¼“å­˜ç»Ÿè®¡ï¼ˆä¾›ç•Œé¢è°ƒç”¨ï¼‰"""
    if not config.CACHE_ENABLED:
        return "Cache disabled"
    
    cache_mgr = get_cache_manager()
    stats = cache_mgr.get_stats()
    
    return f"""ğŸ“Š Cache Statistics:
    
ğŸ’¾ Memory: {stats['memory_cache_size']} entries
ğŸ’½ Disk: {stats['disk_cache_count']} entries ({stats['disk_cache_size_mb']:.1f}MB)

ğŸ“ˆ Performance:
  Total requests: {stats['total_requests']}
  Memory hits: {stats['memory_hits']} âš¡
  Disk hits: {stats['disk_hits']} ğŸ’¾
  Misses: {stats['misses']} âŒ
  
ğŸ¯ Hit rate: {stats['hit_rate']}"""


def clear_cache():
    """æ¸…ç©ºç¼“å­˜ï¼ˆä¾›ç•Œé¢è°ƒç”¨ï¼‰"""
    if not config.CACHE_ENABLED:
        return "Cache disabled"
    
    cache_mgr = get_cache_manager()
    cache_mgr.clear_all()
    return "âœ… Cache cleared successfully!"