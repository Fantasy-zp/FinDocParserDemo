"""
å·¥å…·å‡½æ•°ï¼šPDFå¤„ç† + æ¨¡å‹æ¨ç† - Phase 3.2 æµå¼ç‰ˆæœ¬
"""
import base64
from pathlib import Path
from PIL import Image
import fitz  # PyMuPDF
from openai import OpenAI
import config
from io import BytesIO
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
from typing import Generator, Dict, Any, List, Tuple
from cache_manager import get_cache_manager


def pdf_to_images(pdf_path):
    """PDF è½¬å›¾ç‰‡"""
    doc = fitz.open(pdf_path)
    images = []
    
    for page_num in range(len(doc)):
        page = doc[page_num]
        mat = fitz.Matrix(config.PDF_DPI / 72, config.PDF_DPI / 72)
        pix = page.get_pixmap(matrix=mat)
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        images.append(img)
    
    doc.close()
    return images


def resize_image_if_needed(image):
    """è°ƒæ•´å›¾åƒå¤§å°"""
    width, height = image.size
    pixels = width * height
    
    if pixels > config.IMAGE_MAX_PIXELS:
        scale = (config.IMAGE_MAX_PIXELS / pixels) ** 0.5
        new_width = int(width * scale)
        new_height = int(height * scale)
        image = image.resize((new_width, new_height), Image.LANCZOS)
    
    return image


def image_to_base64(image):
    """å›¾ç‰‡è½¬ Base64"""
    if isinstance(image, (str, Path)):
        image = Image.open(image)
    
    image = resize_image_if_needed(image)
    
    buffer = BytesIO()
    if image.mode in ('RGBA', 'LA', 'P'):
        image = image.convert('RGB')
    image.save(buffer, format='JPEG', quality=85)
    
    return base64.b64encode(buffer.getvalue()).decode()


def infer_single_image(image, model_key, prompt, temperature, top_p, max_tokens):
    """å•å¼ å›¾ç‰‡æ¨ç†"""
    if model_key not in config.MODELS:
        raise ValueError(f"æœªçŸ¥çš„æ¨¡å‹: {model_key}")
    
    model_config = config.MODELS[model_key]
    img_base64 = image_to_base64(image)
    
    client = OpenAI(
        api_key="dummy",
        base_url=model_config["api_base"],
        timeout=120.0
    )
    
    response = client.chat.completions.create(
        model=model_config["model_id"],
        messages=[{
            "role": "user",
            "content": [
                {
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/jpeg;base64,{img_base64}",
                        "detail": "high"
                    }
                },
                {
                    "type": "text",
                    "text": prompt
                }
            ]
        }],
        max_tokens=max_tokens,
        temperature=temperature,
        top_p=top_p,
        extra_body={
            'repetition_penalty': 1.0,
            'top_k': 50,
            'skip_special_tokens': True,
        }
    )
    # è·å–è¿”å›å†…å®¹
    result = response.choices[0].message.content

    # âœ… æ·»åŠ  JSON æ ¼å¼åˆ¤æ–­å’Œæå–
    result = extract_markdown_from_result(result)
    
    return result

# âœ… æ·»åŠ æ–°å‡½æ•°ï¼šæå– markdown å†…å®¹
def extract_markdown_from_result(result: str) -> str:
    """
    ä» API è¿”å›ç»“æœä¸­æå– markdown å†…å®¹ï¼ˆå®Œæ•´å¢å¼ºç‰ˆï¼‰
    
    æ”¯æŒï¼š
    1. å®Œæ•´ JSON â†’ json.loads è§£æ
    2. ä¸å®Œæ•´ JSON â†’ æ­£åˆ™æå–
    3. çº¯ Markdown â†’ ç›´æ¥è¿”å›
    """
    import json
    import re
    
    original_result = result
    result = result.strip()
    
    # å¿«é€Ÿæ’é™¤ï¼šæ˜æ˜¾ä¸æ˜¯ JSON
    if not result.startswith('{'):
        return original_result
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å« natural_text å­—æ®µ
    if '"natural_text"' not in result:
        print("â„¹ï¸  JSON æ ¼å¼ä½†ä¸åŒ…å« natural_text å­—æ®µ")
        return original_result
    
    # ============================================
    # å°è¯• 1ï¼šå®Œæ•´ JSON è§£æ
    # ============================================
    try:
        data = json.loads(result)
        
        if isinstance(data, dict) and 'natural_text' in data:
            markdown_text = data['natural_text']
            print(f"âœ… ä»å®Œæ•´ JSON æå– natural_text (é•¿åº¦: {len(markdown_text)} å­—ç¬¦)")
            return f"{markdown_text}\n\n<!-- RAW_OUTPUT_START\n{original_result}\nRAW_OUTPUT_END -->"
        else:
            print(f"âš ï¸  JSON è§£ææˆåŠŸä½†ç»“æ„ä¸ç¬¦åˆé¢„æœŸ")
            return original_result
            
    except json.JSONDecodeError as e:
        # ============================================
        # å°è¯• 2ï¼šä»ä¸å®Œæ•´ JSON ä¸­æ­£åˆ™æå–
        # ============================================
        print(f"âš ï¸  JSON ä¸å®Œæ•´ï¼Œå°è¯•æ­£åˆ™æå–...")
        print(f"   é”™è¯¯ä¿¡æ¯: {str(e)}")
        print(f"   æœ€å 50 å­—ç¬¦: ...{result[-50:]}")
        
        # ç”¨æ­£åˆ™æå– natural_text çš„å€¼
        extracted = extract_natural_text_by_regex(result)
        
        if extracted:
            print(f"âš¡ æˆåŠŸä»ä¸å®Œæ•´ JSON ä¸­æå– {len(extracted)} å­—ç¬¦")
            
            # æ·»åŠ æˆªæ–­è­¦å‘Š
            # warning = "\n\n---\nâš ï¸ **è­¦å‘Š**ï¼šè¾“å‡ºè¢«æˆªæ–­ï¼ˆè¾¾åˆ° Max Tokens é™åˆ¶ï¼‰ï¼Œå†…å®¹å¯èƒ½ä¸å®Œæ•´ã€‚å»ºè®®å¢åŠ  Max Tokens å‚æ•°ã€‚"
            
            return f"{extracted}\n\n<!-- RAW_OUTPUT_START\n{original_result}\nRAW_OUTPUT_END -->"
        else:
            print(f"âŒ æ— æ³•ä»ä¸å®Œæ•´ JSON ä¸­æå– natural_text")
            return f"âš ï¸ **è§£æå¤±è´¥**ï¼šè¾“å‡ºè¢«æˆªæ–­ä¸”æ— æ³•æå–å†…å®¹ã€‚\n\n**å»ºè®®**ï¼š\n1. å¢åŠ  Max Tokens åˆ° 16384\n2. ç®€åŒ–æ–‡æ¡£æˆ–åˆ†é¡µå¤„ç†\n\n**åŸå§‹è¾“å‡º**ï¼š\n```\n{original_result[:500]}...\n```"


def extract_natural_text_by_regex(incomplete_json: str) -> str:
    """
    ç”¨æ­£åˆ™ä»ä¸å®Œæ•´çš„ JSON ä¸­æå– natural_text çš„å€¼
    
    Args:
        incomplete_json: ä¸å®Œæ•´çš„ JSON å­—ç¬¦ä¸²
    
    Returns:
        æå–çš„ markdown å†…å®¹ï¼ˆå¯èƒ½ä¸å®Œæ•´ï¼‰
    """
    import re
    
    # åŒ¹é…æ¨¡å¼ï¼ˆæŒ‰ä¼˜å…ˆçº§å°è¯•ï¼‰
    patterns = [
        # æ¨¡å¼ 1ï¼šå®Œæ•´çš„å€¼ï¼ˆå¸¦ç»“æŸå¼•å·å’Œé€—å·/å³æ‹¬å·ï¼‰
        r'"natural_text"\s*:\s*"((?:[^"\\]|\\.)*)"',
        
        # æ¨¡å¼ 2ï¼šä¸å®Œæ•´çš„å€¼ï¼ˆæ²¡æœ‰ç»“æŸå¼•å·ï¼Œç›´åˆ°å­—ç¬¦ä¸²æœ«å°¾ï¼‰
        r'"natural_text"\s*:\s*"((?:[^"\\]|\\.)*?)(?:$|")',
    ]
    
    for i, pattern in enumerate(patterns, 1):
        match = re.search(pattern, incomplete_json, re.DOTALL)
        
        if match:
            content = match.group(1)
            
            # å¤„ç† JSON è½¬ä¹‰å­—ç¬¦
            content = unescape_json_string(content)
            
            print(f"   âœ… æ­£åˆ™æ¨¡å¼ {i} åŒ¹é…æˆåŠŸ")
            return content.strip()
    
    return ""


def unescape_json_string(s: str) -> str:
    """å¤„ç† JSON å­—ç¬¦ä¸²ä¸­çš„è½¬ä¹‰å­—ç¬¦"""
    # æŒ‰é¡ºåºå¤„ç†ï¼ˆé¡ºåºå¾ˆé‡è¦ï¼‰
    replacements = [
        ('\\n', '\n'),   # æ¢è¡Œ
        ('\\t', '\t'),   # åˆ¶è¡¨ç¬¦
        ('\\r', '\r'),   # å›è½¦
        ('\\"', '"'),    # å¼•å·
        ('\\/', '/'),    # æ–œæ 
        ('\\\\', '\\'),  # åæ–œæ ï¼ˆæœ€åå¤„ç†ï¼‰
    ]
    
    for old, new in replacements:
        s = s.replace(old, new)
    
    return s

# def extract_markdown_from_result(result: str) -> str:
#     """å¢å¼ºç‰ˆï¼šæ”¯æŒå¤šç§æå–ç­–ç•¥"""
#     import json
    
#     result = result.strip()
    
#     # ç­–ç•¥ 1ï¼šJSON æ ¼å¼
#     if result.startswith('{') and result.endswith('}'):
#         try:
#             data = json.loads(result)
            
#             if isinstance(data, dict):
#                 # ä¼˜å…ˆçº§ï¼šnatural_text > text > content > markdown
#                 for key in ['natural_text', 'text', 'content', 'markdown']:
#                     if key in data:
#                         print(f"âœ… ä» JSON æå–å­—æ®µ: {key}")
#                         return data[key]
#         except json.JSONDecodeError:
#             pass
    
#     # ç­–ç•¥ 2ï¼šMarkdown ä»£ç å—
#     if '```markdown' in result:
#         import re
#         match = re.search(r'```markdown\n(.*?)\n```', result, re.DOTALL)
#         if match:
#             print("âœ… ä» markdown ä»£ç å—æå–")
#             return match.group(1)
    
#     # é»˜è®¤ï¼šè¿”å›åŸå§‹ç»“æœ
#     return result

def process_single_page_with_index(idx, image, model_key, prompt, temperature, top_p, max_tokens):
    """å¤„ç†å•é¡µï¼ˆå¸¦ç´¢å¼•ï¼‰"""
    start_time = time.time()
    try:
        result = infer_single_image(image, model_key, prompt, temperature, top_p, max_tokens)
        elapsed = time.time() - start_time
        return (idx, result, elapsed, None)
    except Exception as e:
        elapsed = time.time() - start_time
        return (idx, None, elapsed, str(e))


# ============================================
# Phase 3.2: æµå¼å¤„ç†æ ¸å¿ƒå‡½æ•°
# ============================================

def process_images_streaming(
    images: List[Image.Image],
    model_key: str,
    prompt: str,
    temperature: float,
    top_p: float,
    max_tokens: int
) -> Generator[Dict[str, Any], None, None]:
    """
    æµå¼å¹¶è¡Œå¤„ç†å›¾ç‰‡ï¼ˆPhase 3.4 ä¼˜åŒ–ç‰ˆ - å•é¡µå®æ—¶åé¦ˆå¢å¼ºï¼‰
    
    å•é¡µå¤„ç†æ—¶ä¼šå®æ—¶æ›´æ–°å·²ç”¨æ—¶é—´
    """
    import threading
    
    total = len(images)
    completed_count = 0
    elapsed_times = []
    start_time = time.time()
    
    # å­˜å‚¨ç»“æœï¼ˆä¿æŒé¡ºåºï¼‰
    results = {}
    
    # å•é¡µç›´æ¥å¤„ç†ï¼ˆâœ… æ·»åŠ å®æ—¶è¿›åº¦æ›´æ–°ï¼‰
    if total < config.PARALLEL_MIN_PAGES:
        for idx, img in enumerate(images):
            # ç”¨äºçº¿ç¨‹é—´é€šä¿¡
            result_container = {"result": None, "elapsed": 0, "error": None, "done": False}
            
            # âœ… åœ¨ç‹¬ç«‹çº¿ç¨‹ä¸­æ‰§è¡Œæ¨ç†
            def inference_thread():
                page_start = time.time()
                try:
                    _, result, page_elapsed, error = process_single_page_with_index(
                        idx, img, model_key, prompt, temperature, top_p, max_tokens
                    )
                    result_container["result"] = result
                    result_container["elapsed"] = page_elapsed
                    result_container["error"] = error
                except Exception as e:
                    result_container["error"] = str(e)
                finally:
                    result_container["done"] = True
            
            # å¯åŠ¨æ¨ç†çº¿ç¨‹
            thread = threading.Thread(target=inference_thread, daemon=True)
            thread.start()
            
            # âœ… ä¸»çº¿ç¨‹å®šæœŸæ›´æ–°çŠ¶æ€ï¼ˆæ¯0.5ç§’ï¼‰
            page_start_time = time.time()
            while not result_container["done"]:
                elapsed = time.time() - page_start_time
                
                # æ¨¡æ‹Ÿè¿›åº¦ï¼ˆåŸºäºæ—¶é—´ä¼°ç®—ï¼‰
                # å‡è®¾å¹³å‡æ¯é¡µ 5-10 ç§’ï¼Œç”¨è„‰æåŠ¨ç”»
                pulse = int((elapsed * 2) % 20)  # 0-19 å¾ªç¯
                progress_bar = "â–ˆ" * pulse + "â–‘" * (20 - pulse)
                
                yield {
                    "page_num": idx + 1,
                    "total_pages": total,
                    "result": None,
                    "completed": completed_count,
                    "progress": 0,
                    "elapsed": elapsed,
                    "eta": 0,
                    "status": f"â³ æ­£åœ¨å¤„ç†ç¬¬ {idx + 1}/{total} é¡µ..."
                }
                
                time.sleep(0.5)  # æ¯0.5ç§’æ›´æ–°ä¸€æ¬¡
            
            # âœ… ç­‰å¾…çº¿ç¨‹å®Œæˆ
            thread.join(timeout=1)
            
            # å¤„ç†ç»“æœ
            completed_count += 1
            elapsed_times.append(result_container["elapsed"])
            
            if result_container["error"] is None:
                results[idx] = result_container["result"]
            else:
                results[idx] = f"Error: {result_container['error']}"
            
            # âœ… è¿”å›å®ŒæˆçŠ¶æ€
            yield {
                "page_num": idx + 1,
                "total_pages": total,
                "result": results[idx],
                "completed": completed_count,
                "progress": 1.0,
                "elapsed": result_container["elapsed"],
                "eta": 0,
                "status": f"âœ… ç¬¬ {idx + 1}/{total} é¡µå®Œæˆ ({result_container['elapsed']:.1f}s)"
            }
        return
    
    # å¤šé¡µå¹¶è¡Œå¤„ç†ï¼ˆä¿æŒä¸å˜ï¼‰
    with ThreadPoolExecutor(max_workers=config.MAX_WORKERS) as executor:
        future_to_idx = {
            executor.submit(
                process_single_page_with_index,
                idx, img, model_key, prompt, temperature, top_p, max_tokens
            ): idx
            for idx, img in enumerate(images)
        }
        
        for future in as_completed(future_to_idx):
            idx, result, page_elapsed, error = future.result()
            completed_count += 1
            elapsed_times.append(page_elapsed)
            
            results[idx] = result if error is None else f"Error on page {idx + 1}: {error}"
            
            avg_time = sum(elapsed_times) / len(elapsed_times)
            remaining = total - completed_count
            eta = avg_time * remaining
            total_elapsed = time.time() - start_time
            
            yield {
                "page_num": idx + 1,
                "total_pages": total,
                "result": results[idx],
                "completed": completed_count,
                "progress": completed_count / total,
                "elapsed": total_elapsed,
                "eta": eta,
                "status": f"âœ… ç¬¬ {idx + 1}/{total} é¡µå®Œæˆ ({page_elapsed:.1f}s, é¢„è®¡å‰©ä½™: {eta:.1f}s)"
            }

def merge_results_ordered(results: Dict[int, str], total: int) -> str:
    """
    æŒ‰é¡ºåºåˆå¹¶ç»“æœ
    
    Args:
        results: {é¡µç ç´¢å¼•: markdownç»“æœ}
        total: æ€»é¡µæ•°
    
    Returns:
        åˆå¹¶åçš„ markdown
    """
    ordered_results = []
    for i in range(total):
        if i in results:
            ordered_results.append(f"## Page {i + 1}\n\n{results[i]}")
        else:
            ordered_results.append(f"## Page {i + 1}\n\nâ³ Processing...")
    
    return "\n\n---\n\n".join(ordered_results)


def process_document_streaming(
    file_path: str,
    model_key: str,
    prompt: str,
    temperature: float,
    top_p: float,
    max_tokens: int
) -> Generator[Tuple[List[Image.Image], str, str], None, None]:
    """
    æµå¼å¤„ç†æ–‡æ¡£ï¼ˆPhase 3.2 ä¸»æ¥å£ï¼‰
    
    æ¯å®Œæˆä¸€é¡µå°±è¿”å›å½“å‰çŠ¶æ€
    
    Args:
        file_path: æ–‡ä»¶è·¯å¾„
        å…¶ä»–å‚æ•°: æ¨ç†å‚æ•°
    
    Yields:
        (images, status, markdown)
        - images: å·²å¤„ç†çš„å›¾ç‰‡åˆ—è¡¨
        - status: çŠ¶æ€æ–‡æœ¬
        - markdown: å½“å‰ç´¯ç§¯çš„ç»“æœ
    """
    file_path = Path(file_path)
    
    # åŠ è½½å›¾ç‰‡
    if file_path.suffix.lower() == '.pdf':
        images = pdf_to_images(file_path)
    else:
        images = [Image.open(file_path)]
    
    total = len(images)
    
    # åˆå§‹çŠ¶æ€
    yield (
        images,
        f"ğŸ“„ Loaded {total} page(s), starting processing...",
        ""
    )
    
    # æ”¶é›†æ‰€æœ‰ç»“æœ
    all_results = {}
    
    # æµå¼å¤„ç†
    for update in process_images_streaming(
        images, model_key, prompt, temperature, top_p, max_tokens
    ):
        page_idx = update["page_num"] - 1
        all_results[page_idx] = update["result"]
        
        # æ„å»ºçŠ¶æ€æ–‡æœ¬
        progress_bar = "â–ˆ" * int(update["progress"] * 20) + "â–‘" * (20 - int(update["progress"] * 20))
        status = f"""ğŸ”„ Processing: {update['completed']}/{update['total_pages']} pages

{progress_bar} {update['progress']*100:.1f}%

â±ï¸  Elapsed: {update['elapsed']:.1f}s
â° ETA: {update['eta']:.1f}s

{update['status']}"""
        
        # åˆå¹¶å½“å‰ç»“æœ
        if total == 1:
            markdown = all_results.get(0, "")
        else:
            markdown = merge_results_ordered(all_results, total)
        
        # âœ… è¿”å›å½“å‰çŠ¶æ€
        yield (images, status, markdown)
    
    # æœ€ç»ˆçŠ¶æ€
    final_markdown = merge_results_ordered(all_results, total) if total > 1 else all_results.get(0, "")
    
    yield (
        images,
        f"âœ… Completed! {total} page(s) processed successfully.",
        final_markdown
    )


# ============================================
# ä¿ç•™åŸæœ‰æ¥å£ï¼ˆå‘ä¸‹å…¼å®¹ï¼‰
# ============================================

def process_images_parallel(images, model_key, prompt, temperature, top_p, max_tokens, progress=None):
    """å¹¶è¡Œå¤„ç†ï¼ˆéæµå¼ï¼Œä¿ç•™å…¼å®¹ï¼‰"""
    total = len(images)
    
    if total < config.PARALLEL_MIN_PAGES:
        results = []
        for idx, img in enumerate(images):
            if progress is not None:
                progress((idx + 1) / total, desc=f"Processing page {idx + 1}/{total}")
            
            _, result, elapsed, error = process_single_page_with_index(
                idx, img, model_key, prompt, temperature, top_p, max_tokens
            )
            results.append(result if error is None else f"Error: {error}")
        return results
    
    results = [None] * total
    completed_count = 0
    total_time = 0
    
    with ThreadPoolExecutor(max_workers=config.MAX_WORKERS) as executor:
        future_to_idx = {
            executor.submit(
                process_single_page_with_index,
                idx, img, model_key, prompt, temperature, top_p, max_tokens
            ): idx
            for idx, img in enumerate(images)
        }
        
        for future in as_completed(future_to_idx):
            idx, result, elapsed, error = future.result()
            completed_count += 1
            total_time += elapsed
            
            if error is None:
                results[idx] = result
            else:
                results[idx] = f"Error on page {idx + 1}: {error}"
            
            if progress is not None:
                avg_time = total_time / completed_count
                remaining = total - completed_count
                eta = avg_time * remaining
                progress(
                    completed_count / total,
                    desc=f"Completed {completed_count}/{total} pages (ETA: {eta:.1f}s)"
                )
    
    return results


def process_document(file_path, model_key, prompt, temperature, top_p, max_tokens, progress=None):
    """å¤„ç†æ–‡æ¡£ï¼ˆéæµå¼ï¼Œä¿ç•™å…¼å®¹ï¼‰"""
    file_path = Path(file_path)
    
    if file_path.suffix.lower() == '.pdf':
        if progress is not None:
            progress(0, desc="Converting PDF to images...")
        images = pdf_to_images(file_path)
    else:
        images = [Image.open(file_path)]
    
    if config.PARALLEL_ENABLED:
        results = process_images_parallel(
            images, model_key, prompt, temperature, top_p, max_tokens, progress
        )
    else:
        results = []
        for idx, img in enumerate(images):
            if progress is not None:
                progress((idx + 1) / len(images), desc=f"Processing page {idx + 1}/{len(images)}")
            result = infer_single_image(img, model_key, prompt, temperature, top_p, max_tokens)
            results.append(result)
    
    if len(results) > 1:
        markdown = "\n\n---\n\n".join([
            f"## Page {i + 1}\n\n{result}" 
            for i, result in enumerate(results)
        ])
    else:
        markdown = results[0]
    
    return images, markdown


def validate_file(file_path):
    """éªŒè¯æ–‡ä»¶"""
    if not file_path:
        return False, "è¯·ä¸Šä¼ æ–‡ä»¶"
    
    file_path = Path(file_path)
    
    if not file_path.exists():
        return False, "æ–‡ä»¶ä¸å­˜åœ¨"
    
    if file_path.suffix.lower() not in config.ALLOWED_FILE_TYPES:
        return False, f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹ï¼Œä»…æ”¯æŒï¼š{', '.join(config.ALLOWED_FILE_TYPES)}"
    
    size_mb = file_path.stat().st_size / (1024 * 1024)
    if size_mb > config.MAX_FILE_SIZE_MB:
        return False, f"æ–‡ä»¶è¿‡å¤§ï¼ˆ{size_mb:.1f}MBï¼‰ï¼Œæœ€å¤§æ”¯æŒ {config.MAX_FILE_SIZE_MB}MB"
    
    return True, ""


def get_model_choices():
    """è·å–æ¨¡å‹é€‰æ‹©åˆ—è¡¨"""
    return [model["name"] for model in config.MODELS.values()]


def get_model_key_from_name(model_name):
    """ä»æ˜¾ç¤ºåç§°è·å–æ¨¡å‹é”®"""
    for key, model in config.MODELS.items():
        if model["name"] == model_name:
            return key
    return config.DEFAULT_MODEL


def test_model_connection(model_key):
    """æµ‹è¯•æ¨¡å‹ API è¿æ¥"""
    try:
        model_config = config.MODELS[model_key]
        client = OpenAI(
            api_key="dummy",
            base_url=model_config["api_base"],
            timeout=5.0
        )
        
        client.models.list()
        return True, f"âœ… {model_config['name']} è¿æ¥æ­£å¸¸"
    
    except Exception as e:
        return False, f"âŒ {model_config['name']} è¿æ¥å¤±è´¥: {str(e)}"

# ============================================
# Phase 3.3: ç¼“å­˜é…ç½®
# ============================================

def process_document_with_cache(
    file_path: str,
    model_key: str,
    prompt: str,
    temperature: float,
    top_p: float,
    max_tokens: int
) -> Tuple[List[Image.Image], str, bool]:
    """
    å¤„ç†æ–‡æ¡£ï¼ˆå¸¦ç¼“å­˜ï¼‰
    
    Returns:
        (images, markdown, from_cache)
        - images: å›¾ç‰‡åˆ—è¡¨
        - markdown: ç»“æœ
        - from_cache: æ˜¯å¦æ¥è‡ªç¼“å­˜
    """
    if not config.CACHE_ENABLED:
        # ç¼“å­˜æœªå¯ç”¨ï¼Œç›´æ¥å¤„ç†
        images, markdown = process_document(
            file_path, model_key, prompt, temperature, top_p, max_tokens
        )
        return images, markdown, False
    
    # è·å–ç¼“å­˜ç®¡ç†å™¨
    cache_mgr = get_cache_manager()
    
    # ç”Ÿæˆç¼“å­˜é”®
    cache_key = cache_mgr.generate_cache_key(
        file_path, model_key, prompt, temperature, top_p, max_tokens
    )
    
    # å°è¯•ä»ç¼“å­˜è·å–
    cached_result = cache_mgr.get(cache_key)
    
    if cached_result is not None:
        # ç¼“å­˜å‘½ä¸­
        # é‡æ–°åŠ è½½å›¾ç‰‡
        file_path = Path(file_path)
        if file_path.suffix.lower() == '.pdf':
            images = pdf_to_images(file_path)
        else:
            images = [Image.open(file_path)]
        
        markdown = cached_result["markdown"]
        return images, markdown, True
    
    # ç¼“å­˜æœªå‘½ä¸­ï¼Œæ‰§è¡Œæ¨ç†
    images, markdown = process_document(
        file_path, model_key, prompt, temperature, top_p, max_tokens
    )
    
    # ä¿å­˜åˆ°ç¼“å­˜
    result = {
        "markdown": markdown,
        "metadata": {
            "pages": len(images),
            "model": model_key,
            "timestamp": time.time()
        }
    }
    
    cache_mgr.set(
        cache_key,
        result,
        Path(file_path).name,
        model_key,
        temperature,
        top_p,
        max_tokens
    )
    
    return images, markdown, False


def process_document_streaming_with_cache(
    file_path: str,
    model_key: str,
    prompt: str,
    temperature: float,
    top_p: float,
    max_tokens: int
) -> Generator[Tuple[List[Image.Image], str, str, bool], None, None]:
    """
    æµå¼å¤„ç†æ–‡æ¡£ï¼ˆå¸¦ç¼“å­˜ - Phase 3.4 ä¼˜åŒ–ç‰ˆ - å®æ—¶è¿›åº¦æ›´æ–°ï¼‰
    
    Yields:
        (images, status, markdown, from_cache)
    """
    if not config.CACHE_ENABLED:
        for images, status, markdown in process_document_streaming(
            file_path, model_key, prompt, temperature, top_p, max_tokens
        ):
            yield images, status, markdown, False
        return
    
    cache_mgr = get_cache_manager()
    cache_key = cache_mgr.generate_cache_key(
        file_path, model_key, prompt, temperature, top_p, max_tokens
    )
    
    # å°è¯•ä»ç¼“å­˜è·å–
    cached_result = cache_mgr.get(cache_key)
    
    if cached_result is not None:
        # ç¼“å­˜å‘½ä¸­
        file_path_obj = Path(file_path)
        if file_path_obj.suffix.lower() == '.pdf':
            images = pdf_to_images(file_path_obj)
        else:
            images = [Image.open(file_path_obj)]
        
        markdown = cached_result["markdown"]
        pages = cached_result["metadata"]["pages"]
        
        status = f"""âš¡ ä»ç¼“å­˜åŠ è½½ï¼

â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 100%

ğŸ“„ é¡µæ•°: {pages} é¡µ
ğŸ”¥ å“åº”æ—¶é—´: <0.1s
ğŸ’¾ ç¼“å­˜å‘½ä¸­ï¼"""
        
        yield images, status, markdown, True
        return
    
    # ç¼“å­˜æœªå‘½ä¸­ - æ‰§è¡Œæµå¼å¤„ç†
    file_path_obj = Path(file_path)
    
    if file_path_obj.suffix.lower() == '.pdf':
        images = pdf_to_images(file_path_obj)
    else:
        images = [Image.open(file_path_obj)]
    
    total = len(images)
    start_time = time.time()
    
    # åˆå§‹çŠ¶æ€
    initial_status = f"ğŸ“„ å·²åŠ è½½ {total} é¡µï¼Œå¼€å§‹å¤„ç†..."
    yield (images, initial_status, "", False)
    
    # æ”¶é›†æ‰€æœ‰ç»“æœ
    all_results = {}
    
    # æµå¼å¤„ç†
    for update in process_images_streaming(
        images, model_key, prompt, temperature, top_p, max_tokens
    ):
        page_idx = update["page_num"] - 1
        
        if update["result"] is not None:
            all_results[page_idx] = update["result"]
        
        # âœ… æ„å»ºçŠ¶æ€æ–‡æœ¬ï¼ˆå®æ—¶æ›´æ–°ç‰ˆï¼‰
        if total == 1:
            # å•é¡µçš„çŠ¶æ€æ˜¾ç¤º
            if update["completed"] == 0:
                # âœ… å¤„ç†ä¸­ï¼ˆå®æ—¶æ›´æ–°æ—¶é—´ï¼‰
                # ä½¿ç”¨è„‰æåŠ¨ç”»è€Œä¸æ˜¯å›ºå®šè¿›åº¦
                pulse = int((update["elapsed"] * 2) % 20)
                progress_bar = "â–ˆ" * pulse + "â–‘" * (20 - pulse)
                
                status = f"""â³ æ­£åœ¨å¤„ç†...

{progress_bar}

â±ï¸  å·²ç”¨æ—¶é—´: {update['elapsed']:.1f}s

{update['status']}"""
            else:
                # å®Œæˆ
                progress_bar = "â–ˆ" * 20
                status = f"""âœ… å¤„ç†å®Œæˆï¼

{progress_bar} 100%

â±ï¸  å¤„ç†æ—¶é—´: {update['elapsed']:.1f}s

{update['status']}"""
        else:
            # å¤šé¡µçš„çŠ¶æ€æ˜¾ç¤º
            progress_bar = "â–ˆ" * int(update["progress"] * 20) + "â–‘" * (20 - int(update["progress"] * 20))
            status = f"""ğŸ”„ å¤„ç†ä¸­: {update['completed']}/{update['total_pages']} é¡µ

{progress_bar} {update['progress']*100:.1f}%

â±ï¸  å·²ç”¨æ—¶é—´: {update['elapsed']:.1f}s
â° é¢„è®¡å‰©ä½™: {update['eta']:.1f}s

{update['status']}"""
        
        # åˆå¹¶å½“å‰ç»“æœ
        if total == 1:
            if 0 in all_results:
                markdown = all_results[0]
            else:
                markdown = "â³ æ­£åœ¨å¤„ç†ä¸­..."
        else:
            markdown = merge_results_ordered(all_results, total)
        
        yield (images, status, markdown, False)
    
    # å¤„ç†å®Œæˆ
    final_markdown = merge_results_ordered(all_results, total) if total > 1 else all_results.get(0, "")
    total_elapsed = time.time() - start_time
    
    if total == 1:
        final_status = f"""âœ… è§£æå®Œæˆï¼

â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 100%

ğŸ“„ é¡µæ•°: 1 é¡µ
â±ï¸  å¤„ç†æ—¶é—´: {total_elapsed:.1f}s
ğŸ’¾ å·²ä¿å­˜åˆ°ç¼“å­˜"""
    else:
        final_status = f"""âœ… è§£æå®Œæˆï¼

â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 100%

ğŸ“„ æ€»é¡µæ•°: {total} é¡µ
â±ï¸  æ€»è€—æ—¶: {total_elapsed:.1f}s
ğŸ’¾ å·²ä¿å­˜åˆ°ç¼“å­˜"""
    
    # ä¿å­˜åˆ°ç¼“å­˜
    if images is not None:
        result = {
            "markdown": final_markdown,
            "metadata": {
                "pages": len(images),
                "model": model_key,
                "timestamp": time.time()
            }
        }
        
        cache_mgr.set(
            cache_key,
            result,
            file_path_obj.name,
            model_key,
            temperature,
            top_p,
            max_tokens
        )
    
    yield (images, final_status, final_markdown, False)


def get_cache_stats():
    """è·å–ç¼“å­˜ç»Ÿè®¡ï¼ˆä¾›ç•Œé¢è°ƒç”¨ï¼‰"""
    if not config.CACHE_ENABLED:
        return "Cache disabled"
    
    cache_mgr = get_cache_manager()
    stats = cache_mgr.get_stats()
    
    return f"""ğŸ“Š Cache Statistics:
    
ğŸ’¾ Memory: {stats['memory_cache_size']} entries
ğŸ’½ Disk: {stats['disk_cache_count']} entries ({stats['disk_cache_size_mb']:.1f}MB)

ğŸ“ˆ Performance:
  Total requests: {stats['total_requests']}
  Memory hits: {stats['memory_hits']} âš¡
  Disk hits: {stats['disk_hits']} ğŸ’¾
  Misses: {stats['misses']} âŒ
  
ğŸ¯ Hit rate: {stats['hit_rate']}"""


def clear_cache():
    """æ¸…ç©ºç¼“å­˜ï¼ˆä¾›ç•Œé¢è°ƒç”¨ï¼‰"""
    if not config.CACHE_ENABLED:
        return "Cache disabled"
    
    cache_mgr = get_cache_manager()
    cache_mgr.clear_all()
    return "âœ… Cache cleared successfully!"